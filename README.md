# csv-to-kafka

Por qualquer motivo, o CSV ainda existe como um formato de interc√¢mbio de dados onipresente. N√£o fica muito mais simples: coloque algum texto simples com campos separados por v√≠rgulas em um arquivo e coloque .csv no final. Se voc√™ se sentir √∫til, pode incluir uma linha de cabe√ßalho com os nomes dos campos.

```
order_id,customer_id,order_total_usd,make,model,delivery_city,delivery_company,delivery_address
1,535,190899.73,Dodge,Ram Wagon B350,Sheffield,DuBuque LLC,2810 Northland Avenue
2,671,33245.53,Volkswagen,Cabriolet,Edinburgh,Bechtelar-VonRueden,1 Macpherson Crossing
```

Neste artigo veremos como carregar esses dados CSV no Kafka, sem precisar escrever nenhum c√≥digo

√â importante ressaltar que n√£o vamos reinventar a roda tentando escrever algum c√≥digo para fazer isso sozinhos - o Kafka Connect (que faz parte do Apache Kafka) j√° existe para fazer tudo isso para n√≥s; s√≥ precisamos do conector apropriado.

Esquemas?
Sim, esquemas. Os arquivos CSV podem n√£o se importar muito com eles, mas os usu√°rios de seus dados no Kafka sim. Idealmente, queremos uma maneira de definir o esquema dos dados que ingerimos para que possam ser armazenados e lidos por qualquer pessoa que queira usar os dados. Para entender por que isso √© t√£o importante, confira:

```
- Microsservi√ßos de Streaming: Contratos e Compatibilidade (InfoQ talk);
- Sim, Virginia, voc√™ realmente precisa de um registro de esquema (blog);
- Esquemas, Contratos e Compatibilidade (blog);
- Plataforma Confluent agora suporta Protobuf, JSON Schema e formatos personalizados (blog).
```

Se voc√™ for definir um esquema na ingest√£o (e espero que o fa√ßa), use Avro, Protobuf ou JSON Schema.

**Observa√ß√£o**
Voc√™ n√£o precisa usar um esquema. Voc√™ pode simplesmente ingerir os dados CSV como est√£o, e eu abordo isso abaixo tamb√©m

### Conector Kafka Connect SpoolDir
O conector Kafka Connect SpoolDir oferece suporte a v√°rios formatos de arquivo simples, incluindo CSV. Obtenha-o no Confluent Hub. Depois de instal√°-lo em seu trabalhador do Kafka Connect, certifique-se de reinici√°-lo para que ele possa busc√°-lo. Voc√™ pode verificar executando:

```
$ curl -s localhost:8083/connector-plugins|jq '.[].class'|egrep 'SpoolDir'

"com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector"
"com.github.jcustenborder.kafka.connect.spooldir.SpoolDirJsonSourceConnector"
"com.github.jcustenborder.kafka.connect.spooldir.SpoolDirLineDelimitedSourceConnector"
"com.github.jcustenborder.kafka.connect.spooldir.SpoolDirSchemaLessJsonSourceConnector"
"com.github.jcustenborder.kafka.connect.spooldir.elf.SpoolDirELFSourceConnector"
```

### Carregando dados do CSV no Kafka e aplicando um esquema
Se voc√™ tiver uma linha de cabe√ßalho com nomes de campo, poder√° aproveit√°-los para definir o esquema no momento da ingest√£o (o que √© uma boa ideia).

Crie o conector:

```
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-00/config \
    -d '{
        "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
        "topic": "orders_spooldir_00",
        "input.path": "/data/unprocessed",
        "finished.path": "/data/processed",
        "error.path": "/data/error",
        "input.file.pattern": ".*\\.csv",
        "schema.generation.enabled":"true",
        "csv.first.row.as.header":"true"
        }'
```

**Observa√ß√£o**
quando voc√™ cria o conector com esta configura√ß√£o, voc√™ precisa execut√°-lo com "csv.first.row.as.header":"true" e um arquivo com cabe√ßalhos j√° colocados pendentes para serem lidos.

Agora v√° at√© um consumidor Kafka e observe nossos dados. Aqui estou usando kafkacat porque √© √≥timo :)

```
$ docker exec kafkacat \
    kafkacat -b kafka:29092 -t orders_spooldir_00 \
             -C -o-1 -J \
             -s key=s -s value=avro -r http://schema-registry:8081 | \
             jq '.payload'
{
  "order_id": {
    "string": "500"
  },
  "customer_id": {
    "string": "424"
  },
  "order_total_usd": {
    "string": "160312.42"
  },
  "make": {
    "string": "Chevrolet"
  },
  "model": {
    "string": "Suburban 1500"
  },
  "delivery_city": {
    "string": "London"
  },
  "delivery_company": {
    "string": "Predovic LLC"
  },
  "delivery_address": {
    "string": "2 Sundown Drive"
  }
}
```

Al√©m disso, no cabe√ßalho da mensagem Kafka est√£o os metadados do pr√≥prio arquivo:

```
$ docker exec kafkacat \
    kafkacat -b kafka:29092 -t orders_spooldir_00 \
             -C -o-1 -J \
             -s key=s -s value=avro -r http://schema-registry:8081 | \
             jq '.headers'
[
  "file.name",
  "orders.csv",
  "file.path",
  "/data/unprocessed/orders.csv",
  "file.length",
  "39102",
  "file.offset",
  "501",
  "file.last.modified",
  "2020-06-17T13:33:50.000Z"
]
```

### Configurando a chave de mensagem
Supondo que voc√™ tenha uma linha de cabe√ßalho para fornecer nomes de campo, voc√™ pode definir schema.generation.key.fields para o nome do(s) campo(s) que gostaria de usar para a chave de mensagem Kafka. Se voc√™ estiver executando isso ap√≥s o primeiro exemplo acima, lembre-se de que o conector realoca seu arquivo, ent√£o voc√™ precisa mov√™-lo de volta para o local input.path para que ele seja processado novamente.

**Observa√ß√£o**
O nome do conector (aqui √© source-csv-spooldir-01) √© usado para rastrear quais arquivos foram processados e o deslocamento dentro deles, portanto, um conector com o mesmo nome n√£o reprocessar√° um arquivo com o mesmo nome e deslocamento inferior j√° processado. Se voc√™ quiser for√ß√°-lo a reprocessar um arquivo, d√™ um novo nome ao conector.

```
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-01/config \
    -d '{
        "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
        "topic": "orders_spooldir_01",
        "input.path": "/data/unprocessed",
        "finished.path": "/data/processed",
        "error.path": "/data/error",
        "input.file.pattern": ".*\\.csv",
        "schema.generation.enabled":"true",
        "schema.generation.key.fields":"order_id",
        "csv.first.row.as.header":"true"
        }'
```

A mensagem Kafka resultante tem o order_id definido como a chave da mensagem:

```
docker exec kafkacat \
    kafkacat -b kafka:29092 -t orders_spooldir_01 -o-1 \
             -C -J \
             -s key=s -s value=avro -r http://schema-registry:8081 | \
             jq '{"key":.key,"payload": .payload}'
{
  "key": "Struct{order_id=3}",
  "payload": {
    "order_id": {
      "string": "3"
    },
    "customer_id": {
      "string": "695"
    },
    "order_total_usd": {
      "string": "155664.90"
    },
    "make": {
      "string": "Toyota"
    },
    "model": {
      "string": "Avalon"
    },
    "delivery_city": {
      "string": "Brighton"
    },
    "delivery_company": {
      "string": "Jacobs, Ebert and Dooley"
    },
    "delivery_address": {
      "string": "4 Loomis Crossing"
    }
  }
}
```

### Alterando os tipos de campo do esquema
O conector faz um bom trabalho ao definir o esquema, mas talvez voc√™ queira substitu√≠-lo. Voc√™ pode declarar tudo de antem√£o usando a configura√ß√£o value.schema, mas talvez esteja satisfeito com ela inferindo todo o esquema, exceto por alguns campos. Aqui voc√™ pode usar o Single Message Transform para munge-lo:

```
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-02/config \
    -d '{
        "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
        "topic": "orders_spooldir_02",
        "input.path": "/data/unprocessed",
        "finished.path": "/data/processed",
        "error.path": "/data/error",
        "input.file.pattern": ".*\\.csv",
        "schema.generation.enabled":"true",
        "schema.generation.key.fields":"order_id",
        "csv.first.row.as.header":"true",
        "transforms":"castTypes",
        "transforms.castTypes.type":"org.apache.kafka.connect.transforms.Cast$Value",
        "transforms.castTypes.spec":"order_id:int32,customer_id:int32,order_total_usd:float32"
        }'
```

Se voc√™ examinar o esquema que foi criado e armazenado no Registro de Esquema, poder√° ver que os tipos de dados de campo foram definidos conforme especificado:

```
‚ûú curl --silent --location --request GET 'http://localhost:8081/subjects/orders_spooldir_02-value/versions/latest' |jq '.schema|fromjson'
{
  "type": "record", "name": "Value", "namespace": "com.github.jcustenborder.kafka.connect.model",
  "fields": [
    { "name": "order_id", "type": [ "null", "int" ], "default": null },
    { "name": "customer_id", "type": [ "null", "int" ], "default": null },
    { "name": "order_total_usd", "type": [ "null", "float" ], "default": null },
    { "name": "make", "type": [ "null", "string" ], "default": null },
    { "name": "model", "type": [ "null", "string" ], "default": null },
    { "name": "delivery_city", "type": [ "null", "string" ], "default": null },
    { "name": "delivery_company", "type": [ "null", "string" ], "default": null },
    { "name": "delivery_address", "type": [ "null", "string" ], "default": null }
  ],
  "connect.name": "com.github.jcustenborder.kafka.connect.model.Value"
}
```

Apenas gimme o texto simples! üò¢
Todos esses esquemas parecem um monte de barulho, n√£o √©? Bem, na verdade n√£o. Mas, se voc√™ absolutamente deve ter apenas CSV em seu t√≥pico Kafka, veja como. Observe que estamos usando uma classe de conector diferente e estamos usando org.apache.kafka.connect.storage.StringConverter para escrever os valores. 

```
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-03/config \
    -d '{
        "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirLineDelimitedSourceConnector",
        "value.converter":"org.apache.kafka.connect.storage.StringConverter",
        "topic": "orders_spooldir_03",
        "input.path": "/data/unprocessed",
        "finished.path": "/data/processed",
        "error.path": "/data/error",
        "input.file.pattern": ".*\\.csv"
        }'
```

O resultado? Apenas CSV.

```
‚ûú docker exec kafkacat \
    kafkacat -b kafka:29092 -t orders_spooldir_03 -o-5 -C -u -q
496,456,80466.80,Volkswagen,Touareg,Leeds,Hilpert-Williamson,96 Stang Junction
497,210,57743.67,Dodge,Neon,London,Christiansen Group,7442 Algoma Hill
498,88,211171.02,Nissan,370Z,York,"King, Yundt and Skiles",3 1st Plaza
499,343,126072.73,Chevrolet,Camaro,Sheffield,"Schiller, Ankunding and Schumm",8920 Hoffman Place
500,424,160312.42,Chevrolet,Suburban 1500,London,Predovic LLC,2 Sundown Drive
```

### Barra lateral: Esquemas em a√ß√£o
Ent√£o, lemos alguns dados CSV no Kafka. Esse n√£o √© o fim de sua jornada. Vai ser usado para alguma coisa! Vamos fazer isso.

Aqui est√° o ksqlDB, no qual declaramos o t√≥pico de pedidos no qual escrevemos com um esquema como um fluxo:

```
ksql> CREATE STREAM ORDERS_02 WITH (KAFKA_TOPIC='orders_spooldir_02',VALUE_FORMAT='AVRO');

 Message
----------------
 Stream created
----------------
```

Feito isso - e porque existe um esquema que foi criado no momento da ingest√£o - podemos ver todos os campos dispon√≠veis para n√≥s:

```
ksql> DESCRIBE ORDERS_02;

Name                 : ORDERS_02
 Field            | Type
-------------------------------------------
 ROWKEY           | VARCHAR(STRING)  (key)
 ORDER_ID         | INTEGER
 CUSTOMER_ID      | INTEGER
 ORDER_TOTAL_USD  | DOUBLE
 MAKE             | VARCHAR(STRING)
 MODEL            | VARCHAR(STRING)
 DELIVERY_CITY    | VARCHAR(STRING)
 DELIVERY_COMPANY | VARCHAR(STRING)
 DELIVERY_ADDRESS | VARCHAR(STRING)
-------------------------------------------
For runtime statistics and query details run: DESCRIBE EXTENDED <Stream,Table>;
ksql>
```

e execute consultas nos dados que est√£o no Kafka:

```
ksql> SELECT DELIVERY_CITY, COUNT(*) AS ORDER_COUNT, MAX(CAST(ORDER_TOTAL_USD AS DECIMAL(9,2))) AS BIGGEST_ORDER_USD FROM ORDERS_02 GROUP BY DELIVERY_CITY EMIT CHANGES;
+---------------+-------------+---------------------+
|DELIVERY_CITY  |ORDER_COUNT  |BIGGEST_ORDER_USD    |
+---------------+-------------+---------------------+
|Bradford       |13           |189924.47            |
|Edinburgh      |13           |199502.66            |
|Bristol        |16           |213830.34            |
|Sheffield      |74           |216233.98            |
|London         |160          |219736.06            |
```

E quanto aos nossos dados que acabamos de ingerir em um t√≥pico diferente como CSV direto? Porque, tipo, esquemas n√£o s√£o importantes?

```
ksql> CREATE STREAM ORDERS_03 WITH (KAFKA_TOPIC='orders_spooldir_03',VALUE_FORMAT='DELIMITED');
No columns supplied.
```

Sim, sem colunas fornecidas. Sem esquema, sem bueno. Se voc√™ quiser trabalhar com os dados, seja para consultar em SQL, transmitir para um data lake ou fazer qualquer outra coisa, em algum momento voc√™ ter√° que declarar esse esquema. Por isso, o CSV, como m√©todo de serializa√ß√£o sem esquema, √© uma maneira ruim de trocar dados entre sistemas.

Se voc√™ realmente deseja usar seus dados CSV no ksqlDB, voc√™ pode, basta inserir o esquema - que √© propenso a erros e tedioso. Voc√™ o insere toda vez para usar os dados, todos os outros consumidores dos dados tamb√©m o inserem a cada vez. Declar√°-lo uma vez na ingest√£o e estar dispon√≠vel para todos usarem faz muito mais sentido.
